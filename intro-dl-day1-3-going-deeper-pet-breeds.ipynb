{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Classification","metadata":{"id":"TK0xo8T9nqvz"}},{"cell_type":"markdown","source":"Now that you understand what deep learning is, what it's for, and how to create a model, it's time for us to go deeper! In an ideal world deep learning practitioners wouldn't have to know every detail of how things work under the hood… But as yet, we don't live in an ideal world. The truth is, to make your model really work, and work reliably, there are a lot of details you have to get right, and a lot of details that you have to check. This process requires being able to look inside your neural network as it trains, and as it makes predictions, find possible problems, and know how to fix them.\n\nWe will start by repeating the same basic applications that we looked at in the first notebook, but we are going to do two things:\n\n- Make them better.\n- Apply them to a wider variety of types of data.\n\nIn order to do these two things, we will have to learn all of the pieces of the deep learning puzzle. This includes different types of layers, regularization methods, optimizers, how to put layers together into architectures, labeling techniques, and much more. We are not just going to dump all of these things on you, though; we will introduce them progressively as needed, to solve actual problems related to the projects we are working on.","metadata":{"id":"BD20mv8xnqv0"}},{"cell_type":"markdown","source":"## From Dogs and Cats to Pet Breeds","metadata":{"id":"J6pq-9d3nqv1"}},{"cell_type":"markdown","source":"In our very first model we learned how to classify dogs versus cats. Just a few years ago this was considered a very challenging task—but today, it's far too easy! We will not be able to show you the nuances of training models with this problem, because we get a nearly perfect result without worrying about any of the details. But it turns out that the same dataset also allows us to work on a much more challenging problem: figuring out what breed of pet is shown in each image.\n\nIn the intro we presented the applications as already-solved problems. But this is not how things work in real life. We start with some dataset that we know nothing about. We then have to figure out how it is put together, how to extract the data we need from it, and what that data looks like. For the rest of this book we will be showing you how to solve these problems in practice, including all of the intermediate steps necessary to understand the data that you are working with and test your modeling as you go.\n\nWe already downloaded the Pet dataset, and we can get a path to this dataset using the same code as in the intro:","metadata":{"id":"Jp2aXpCPnqv3"}},{"cell_type":"code","source":"from fastai.vision.all import *\npath = untar_data(URLs.PETS)","metadata":{"id":"6zzgxsPEnqv4","execution":{"iopub.status.busy":"2022-04-25T11:25:47.445285Z","iopub.execute_input":"2022-04-25T11:25:47.446169Z","iopub.status.idle":"2022-04-25T11:26:25.134379Z","shell.execute_reply.started":"2022-04-25T11:25:47.446135Z","shell.execute_reply":"2022-04-25T11:26:25.133321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now if we are going to understand how to extract the breed of each pet from each image we're going to need to understand how this data is laid out. Such details of data layout are a vital piece of the deep learning puzzle. Data is usually provided in one of these two ways:\n\n- Individual files representing items of data, such as text documents or images, possibly organized into folders or with filenames representing information about those items\n- A table of data, such as in CSV format, where each row is an item which may include filenames providing a connection between the data in the table and data in other formats, such as text documents and images\n\nThere are exceptions to these rules—particularly in domains such as genomics, where there can be binary database formats or even network streams—but overall the vast majority of the datasets you'll work with will use some combination of these two formats.\n\nTo see what is in our dataset we can use the `ls` method:","metadata":{"id":"XTyDk6KZnqv6"}},{"cell_type":"code","source":"# set this to our base path\nPath.BASE_PATH = path","metadata":{"id":"acTl4A6qnqv8","execution":{"iopub.status.busy":"2022-04-25T11:26:32.25736Z","iopub.execute_input":"2022-04-25T11:26:32.257904Z","iopub.status.idle":"2022-04-25T11:26:32.263545Z","shell.execute_reply.started":"2022-04-25T11:26:32.25787Z","shell.execute_reply":"2022-04-25T11:26:32.262228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path.ls()","metadata":{"id":"OAanxYFDnqv8","outputId":"2ef3b2e6-957e-441a-8d14-2a7063d4778e","execution":{"iopub.status.busy":"2022-04-25T11:26:32.534659Z","iopub.execute_input":"2022-04-25T11:26:32.535146Z","iopub.status.idle":"2022-04-25T11:26:32.550432Z","shell.execute_reply.started":"2022-04-25T11:26:32.53511Z","shell.execute_reply":"2022-04-25T11:26:32.54924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that this dataset provides us with *images* and *annotations* directories. The [website](https://www.robots.ox.ac.uk/~vgg/data/pets/) for the dataset tells us that the *annotations* directory contains information about where the pets are rather than what they are. In this chapter, we will be doing classification, not localization, which is to say that we care about what the pets are, not where they are. Therefore, we will ignore the *annotations* directory for now. So, let's have a look inside the *images* directory:","metadata":{"id":"4yL1S5qrnqv-"}},{"cell_type":"code","source":"(path/\"images\").ls()","metadata":{"id":"2UZDSo3inqv_","outputId":"8994fbc3-8318-4027-f58f-fd327284f920","execution":{"iopub.status.busy":"2022-04-25T11:26:33.344066Z","iopub.execute_input":"2022-04-25T11:26:33.344367Z","iopub.status.idle":"2022-04-25T11:26:33.376348Z","shell.execute_reply.started":"2022-04-25T11:26:33.344331Z","shell.execute_reply":"2022-04-25T11:26:33.375493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most functions and methods in fastai that return a collection use a class called `L`. `L` can be thought of as an enhanced version of the ordinary Python `list` type, with added conveniences for common operations. For instance, when we display an object of this class in a notebook it appears in the format shown there. The first thing that is shown is the number of items in the collection, prefixed with a `#`. You'll also see in the preceding output that the list is suffixed with an ellipsis. This means that only the first few items are displayed—which is a good thing, because we would not want more than 7,000 filenames on our screen!\n\nBy examining these filenames, we can see how they appear to be structured. Each filename contains the pet breed, and then an underscore (`_`), a number, and finally the file extension. We need to create a piece of code that extracts the breed from a single `Path`. Jupyter notebooks make this easy, because we can gradually build up something that works, and then use it for the entire dataset. We do have to be careful to not make too many assumptions at this point. For instance, if you look carefully you may notice that some of the pet breeds contain multiple words, so we cannot simply break at the first `_` character that we find. To allow us to test our code, let's pick out one of these filenames:","metadata":{"id":"pxL5cApPnqwA"}},{"cell_type":"code","source":"fname = (path/\"images\").ls()[0]","metadata":{"id":"--Xx-gpenqwA","execution":{"iopub.status.busy":"2022-04-25T11:26:34.163664Z","iopub.execute_input":"2022-04-25T11:26:34.164022Z","iopub.status.idle":"2022-04-25T11:26:34.193469Z","shell.execute_reply.started":"2022-04-25T11:26:34.163966Z","shell.execute_reply":"2022-04-25T11:26:34.192602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The most powerful and flexible way to extract information from strings like this is to use a *regular expression*, also known as a *regex*. A regular expression is a special string, written in the regular expression language, which specifies a general rule for deciding if another string passes a test (i.e., \"matches\" the regular expression), and also possibly for plucking a particular part or parts out of that other string. \n\nIn this case, we need a regular expression that extracts the pet breed from the filename.\n\nWe do not have the space to give you a complete regular expression tutorial here, but there are many excellent ones online and we know that many of you will already be familiar with this wonderful tool. If you're not, that is totally fine—this is a great opportunity for you to rectify that! We find that regular expressions are one of the most useful tools in our programming toolkit, and many of our students tell us that this is one of the things they are most excited to learn about. So head over to Google and search for \"regular expressions tutorial\" now, and then come back here after you've had a good look around. The [book's website](https://book.fast.ai/) also provides a list of our favorites.\n\n> Not only are regular expressions dead handy, but they also have interesting roots. They are \"regular\" because they were originally examples of a \"regular\" language, the lowest rung within the Chomsky hierarchy, a grammar classification developed by linguist Noam Chomsky, who also wrote _Syntactic Structures_, the pioneering work searching for the formal grammar underlying human language. This is one of the charms of computing: it may be that the hammer you reach for every day in fact came from a spaceship.\n\nWhen you are writing a regular expression, the best way to start is just to try it against one example at first. Let's use the `findall` method to try a regular expression against the filename of the `fname` object:","metadata":{"id":"vJO2IR7snqwA"}},{"cell_type":"code","source":"re.findall(r'(.+)_\\d+.[jpn]{2}g$', fname.name)","metadata":{"id":"EabzM877nqwC","outputId":"11bf33e1-e6ed-49b3-c3b4-0c2666640885","execution":{"iopub.status.busy":"2022-04-25T11:26:34.970851Z","iopub.execute_input":"2022-04-25T11:26:34.971333Z","iopub.status.idle":"2022-04-25T11:26:34.978039Z","shell.execute_reply.started":"2022-04-25T11:26:34.971298Z","shell.execute_reply":"2022-04-25T11:26:34.977011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This regular expression plucks out all the characters leading up to the last underscore character, as long as the subsequence characters are numerical digits and then the JPEG file extension.\n\nNow that we confirmed the regular expression works for the example, let's use it to label the whole dataset. fastai comes with many classes to help with labeling. For labeling with regular expressions, we can use the `RegexLabeller` class. In this example we use the data block API (in fact, we nearly always use the data block API—it's so much more flexible than the simple factory methods we saw in the intro):","metadata":{"id":"HBHL19rUnqwC"}},{"cell_type":"code","source":"pets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.[jpn]{2}g$'), 'name'),\n                 item_tfms=Resize(460),\n                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\ndls = pets.dataloaders(path/\"images\", device= torch.device('cpu'))  # add to cpu as does not work on kaggle on GPU (MAGMA not installed)","metadata":{"id":"nL7c5e-nnqwD","execution":{"iopub.status.busy":"2022-04-25T11:27:01.17149Z","iopub.execute_input":"2022-04-25T11:27:01.171787Z","iopub.status.idle":"2022-04-25T11:27:01.909804Z","shell.execute_reply.started":"2022-04-25T11:27:01.171755Z","shell.execute_reply":"2022-04-25T11:27:01.908789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One important piece of this `DataBlock` call that we haven't seen before is in these two lines:\n\n```python\nitem_tfms=Resize(460),\nbatch_tfms=aug_transforms(size=224, min_scale=0.75)\n```\n\nThese lines implement a fastai data augmentation strategy which we call *presizing*. Presizing is a particular way to do image augmentation that is designed to minimize data destruction while maintaining good performance.","metadata":{"id":"jR3u4rCcnqwD"}},{"cell_type":"markdown","source":"A `DataLoaders` includes validation and training `DataLoader`s. `DataLoader` is a class that provides batches of a few items at a time to the GPU. We'll be learning a lot more about this class in the next chapter. When you loop through a `DataLoader` fastai will give you 64 (by default) items at a time, all stacked up into a single tensor. We can take a look at a few of those items by calling the `show_batch` method on a `DataLoader`:","metadata":{"id":"rlfmIpELnkZM"}},{"cell_type":"code","source":"dls.valid.show_batch(max_n=4, nrows=1)","metadata":{"id":"fhcI9rpUnkZM","outputId":"f2354ebe-b42c-4b72-f65f-081649974fd2","execution":{"iopub.status.busy":"2022-04-25T11:27:04.400307Z","iopub.execute_input":"2022-04-25T11:27:04.400914Z","iopub.status.idle":"2022-04-25T11:27:07.613824Z","shell.execute_reply.started":"2022-04-25T11:27:04.400878Z","shell.execute_reply":"2022-04-25T11:27:07.612855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By default `Resize` *crops* the images to fit a square shape of the size requested, using the full width or height. This can result in losing some important details. Alternatively, you can ask fastai to pad the images with zeros (black), or squish/stretch them:","metadata":{"id":"XUPyG9WXnkZN"}},{"cell_type":"code","source":"pets_new = pets.new(item_tfms=Resize(128, ResizeMethod.Squish))\ndls_new = pets_new.dataloaders(path/\"images\")\ndls_new.valid.show_batch(max_n=4, nrows=1)","metadata":{"id":"JX9ToLcwnkZN","outputId":"1bb58eca-f99b-4926-f08d-87564737c38d","execution":{"iopub.status.busy":"2022-04-25T11:27:07.61563Z","iopub.execute_input":"2022-04-25T11:27:07.61594Z","iopub.status.idle":"2022-04-25T11:27:08.582498Z","shell.execute_reply.started":"2022-04-25T11:27:07.615902Z","shell.execute_reply":"2022-04-25T11:27:08.581615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pets_new = pets.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))\ndls_new = pets_new.dataloaders(path/\"images\")\ndls_new.valid.show_batch(max_n=4, nrows=1)","metadata":{"id":"UgrEqqLnnkZN","outputId":"a2061889-2557-45d4-e235-60daf22a7319","execution":{"iopub.status.busy":"2022-04-25T11:27:08.58413Z","iopub.execute_input":"2022-04-25T11:27:08.584551Z","iopub.status.idle":"2022-04-25T11:27:09.697513Z","shell.execute_reply.started":"2022-04-25T11:27:08.584511Z","shell.execute_reply":"2022-04-25T11:27:09.696462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All of these approaches seem somewhat wasteful, or problematic. If we squish or stretch the images they end up as unrealistic shapes, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy. If we crop the images then we remove some of the features that allow us to perform recognition. For instance, if we were trying to recognize breeds of dog or cat, we might end up cropping out a key part of the body or the face necessary to distinguish between similar breeds. If we pad the images then we have a whole lot of empty space, which is just wasted computation for our model and results in a lower effective resolution for the part of the image we actually use.\n\nInstead, what we normally do in practice is to randomly select part of the image, and crop to just that part. On each epoch (which is one complete pass through all of our images in the dataset) we randomly select a different part of each image. This means that our model can learn to focus on, and recognize, different features in our images. It also reflects how images work in the real world: different photos of the same thing may be framed in slightly different ways.\n\nIn fact, an entirely untrained neural network knows nothing whatsoever about how images behave. It doesn't even recognize that when an object is rotated by one degree, it still is a picture of the same thing! So actually training the neural network with examples of images where the objects are in slightly different places and slightly different sizes helps it to understand the basic concept of what an object is, and how it can be represented in an image.\n\nHere's another example where we replace `Resize` with `RandomResizedCrop`, which is the transform that provides the behavior we just described. The most important parameter to pass in is `min_scale`, which determines how much of the image to select at minimum each time:","metadata":{"id":"Z0w7ncVnnkZO"}},{"cell_type":"code","source":"pets_new = pets.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\ndls_new = pets_new.dataloaders(path/\"images\")\ndls_new.train.show_batch(max_n=4, nrows=1, unique=True)","metadata":{"id":"KPtI2A-AnkZO","outputId":"7c41cb11-26b0-43fd-b310-a12079d2d83a","execution":{"iopub.status.busy":"2022-04-25T11:27:09.700001Z","iopub.execute_input":"2022-04-25T11:27:09.702656Z","iopub.status.idle":"2022-04-25T11:27:10.578041Z","shell.execute_reply.started":"2022-04-25T11:27:09.702611Z","shell.execute_reply":"2022-04-25T11:27:10.57643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We used `unique=True` to have the same image repeated with different versions of this `RandomResizedCrop` transform. This is a specific example of a more general technique, called data augmentation.","metadata":{"id":"9DGR8-BhnkZP"}},{"cell_type":"markdown","source":"### Data Augmentation","metadata":{"id":"AXQIz6j0nkZP"}},{"cell_type":"markdown","source":"*Data augmentation* refers to creating random variations of our input data, such that they appear different, but do not actually change the meaning of the data. Examples of common data augmentation techniques for images are rotation, flipping, perspective warping, brightness changes and contrast changes. For natural photo images such as the ones we are using here, a standard set of augmentations that we have found work pretty well are provided with the `aug_transforms` function. Because our images are now all the same size, we can apply these augmentations to an entire batch of them using the GPU, which will save a lot of time. To tell fastai we want to use these transforms on a batch, we use the `batch_tfms` parameter (note that we're not using `RandomResizedCrop` in this example, so you can see the differences more clearly; we're also using double the amount of augmentation compared to the default, for the same reason):","metadata":{"id":"j_rP4Gu-nkZP"}},{"cell_type":"code","source":"pets_new = pets.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\ndls_new = pets_new.dataloaders(path/\"images\", device= torch.device('cpu'))\ndls_new.train.show_batch(max_n=8, nrows=2, unique=True)","metadata":{"id":"9zq1na4snkZP","outputId":"8748e7cd-22bf-4cbd-af07-4ce2a7acbf6c","execution":{"iopub.status.busy":"2022-04-25T11:27:10.579918Z","iopub.execute_input":"2022-04-25T11:27:10.580654Z","iopub.status.idle":"2022-04-25T11:27:11.926962Z","shell.execute_reply.started":"2022-04-25T11:27:10.580609Z","shell.execute_reply":"2022-04-25T11:27:11.925463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Presizing","metadata":{"id":"n7QbdW0WnqwE"}},{"cell_type":"markdown","source":"We need our images to have the same dimensions, so that they can collate into tensors to be passed to the GPU. We also want to minimize the number of distinct augmentation computations we perform. The performance requirement suggests that we should, where possible, compose our augmentation transforms into fewer transforms (to reduce the number of computations and the number of lossy operations) and transform the images into uniform sizes (for more efficient processing on the GPU).\n\nThe challenge is that, if performed after resizing down to the augmented size, various common data augmentation transforms might introduce spurious empty zones, degrade data, or both. For instance, rotating an image by 45 degrees fills corner regions of the new bounds with emptiness, which will not teach the model anything. Many rotation and zooming operations will require interpolating to create pixels. These interpolated pixels are derived from the original image data but are still of lower quality.\n\nTo work around these challenges, presizing adopts two strategies that are shown in the figure below:\n\n1. Resize images to relatively \"large\" dimensions—that is, dimensions significantly larger than the target training dimensions. \n1. Compose all of the common augmentation operations (including a resize to the final target size) into one, and perform the combined operation on the GPU only once at the end of processing, rather than performing the operations individually and interpolating multiple times.\n\nThe first step, the resize, creates images large enough that they have spare margin to allow further augmentation transforms on their inner regions without creating empty zones. This transformation works by resizing to a square, using a large crop size. On the training set, the crop area is chosen randomly, and the size of the crop is selected to cover the entire width or height of the image, whichever is smaller.\n\nIn the second step, the GPU is used for all data augmentation, and all of the potentially destructive operations are done together, with a single interpolation at the end.","metadata":{"id":"hWX5O0SLnqwE"}},{"cell_type":"markdown","source":"<img alt=\"Presizing on the training set\" width=\"600\" caption=\"Presizing on the training set\" id=\"presizing\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00060.png?raw=1\">","metadata":{"id":"j09yvlX5nqwF"}},{"cell_type":"markdown","source":"This picture shows the two steps:\n\n1. *Crop full width or height*: This is in `item_tfms`, so it's applied to each individual image before it is copied to the GPU. It's used to ensure all images are the same size. On the training set, the crop area is chosen randomly. On the validation set, the center square of the image is always chosen.\n2. *Random crop and augment*: This is in `batch_tfms`, so it's applied to a batch all at once on the GPU, which means it's fast. On the validation set, only the resize to the final size needed for the model is done here. On the training set, the random crop and any other augmentations are done first.\n\nTo implement this process in fastai you use `Resize` as an item transform with a large size, and `RandomResizedCrop` as a batch transform with a smaller size. `RandomResizedCrop` will be added for you if you include the `min_scale` parameter in your `aug_transforms` function, as was done in the `DataBlock` call in the previous section. Alternatively, you can use `pad` or `squish` instead of `crop` (the default) for the initial `Resize`.\n\nThe code below shows the difference between an image that has been zoomed, interpolated, rotated, and then interpolated again (which is the approach used by all other deep learning libraries), shown here on the right, and an image that has been zoomed and rotated as one operation and then interpolated just once on the left (the fastai approach), shown here on the left.","metadata":{"id":"HFk_YeyJnqwF"}},{"cell_type":"code","source":"!mkdir images\n!wget -O images/grizzly.jpg https://github.com/fastai/fastbook/blob/master/images/grizzly.jpg?raw=true","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:27:13.420757Z","iopub.execute_input":"2022-04-25T11:27:13.421583Z","iopub.status.idle":"2022-04-25T11:27:16.18462Z","shell.execute_reply.started":"2022-04-25T11:27:13.421535Z","shell.execute_reply":"2022-04-25T11:27:16.183522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show difference if you do interpolation once or twice\n\n#id interpolations\n#caption A comparison of fastai's data augmentation strategy (left) and the traditional approach (right).\ndblock1 = DataBlock(blocks=(ImageBlock(), CategoryBlock()),\n                   get_y=parent_label,\n                   item_tfms=Resize(460))\n# Place an image in the 'images/grizzly.jpg' subfolder where this notebook is located before running this\ndls1 = dblock1.dataloaders([(Path.cwd()/'images'/'grizzly.jpg')]*100, bs=8, device= torch.device('cpu'))\ndls1.train.get_idxs = lambda: Inf.ones\nx,y = dls1.valid.one_batch()\n_,axs = subplots(1, 2)\n\nx1 = TensorImage(x.clone())\nx1 = x1.affine_coord(sz=224)\nx1 = x1.rotate(draw=30, p=1.)\nx1 = x1.zoom(draw=1.2, p=1.)\nx1 = x1.warp(draw_x=-0.2, draw_y=0.2, p=1.)\n\ntfms = setup_aug_tfms([Rotate(draw=30, p=1, size=224), Zoom(draw=1.2, p=1., size=224),\n                       Warp(draw_x=-0.2, draw_y=0.2, p=1., size=224)])\nx = Pipeline(tfms)(x)\n#x.affine_coord(coord_tfm=coord_tfm, sz=size, mode=mode, pad_mode=pad_mode)\nTensorImage(x[0]).show(ctx=axs[0])\nTensorImage(x1[0]).show(ctx=axs[1]);","metadata":{"hide_input":false,"id":"0l1SW2VknqwG","outputId":"4603d0f3-911d-46a6-f05f-e551f36607ed","execution":{"iopub.status.busy":"2022-04-25T11:27:16.18722Z","iopub.execute_input":"2022-04-25T11:27:16.187616Z","iopub.status.idle":"2022-04-25T11:27:17.419642Z","shell.execute_reply.started":"2022-04-25T11:27:16.187569Z","shell.execute_reply":"2022-04-25T11:27:17.418382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see that the image on the right is less well defined and has reflection padding artifacts in the bottom-left corner; also, the grass at the top left has disappeared entirely. We find that in practice using presizing significantly improves the accuracy of models, and often results in speedups too.\n\nThe fastai library also provides simple ways to check your data looks right before training a model, which is an extremely important step. We'll look at those next.","metadata":{"id":"zHeU3P1YnqwG"}},{"cell_type":"markdown","source":"### Train simple model","metadata":{"id":"G-yaw0iMnqwG"}},{"cell_type":"markdown","source":"\n\nOnce you think your data looks right, we generally recommend the next step should be using it to train a simple model. We often see people put off the training of an actual model for far too long. As a result, they don't actually find out what their baseline results look like. Perhaps your problem doesn't need lots of fancy domain-specific engineering. Or perhaps the data doesn't seem to train the model at all. These are things that you want to know as soon as possible. For this initial test, we'll use the same simple model that we used in the first notebook:","metadata":{"id":"E_S1LufCnqwJ"}},{"cell_type":"code","source":"pets_new = pets.new(item_tfms=Resize(224))  # disable the aug_transforms as it does not work on kaggle on GPU\ndls_new = pets_new.dataloaders(path/\"images\")","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:27:17.422536Z","iopub.execute_input":"2022-04-25T11:27:17.423299Z","iopub.status.idle":"2022-04-25T11:27:17.565344Z","shell.execute_reply.started":"2022-04-25T11:27:17.423238Z","shell.execute_reply":"2022-04-25T11:27:17.564199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn = vision_learner(dls_new, resnet34, metrics=error_rate)\nlearn.fine_tune(2)","metadata":{"id":"Ni9Xc1rvnqwJ","outputId":"479355a0-6b68-4dc6-d6e6-b8bab9347e9b","execution":{"iopub.status.busy":"2022-04-25T11:27:22.924174Z","iopub.execute_input":"2022-04-25T11:27:22.924539Z","iopub.status.idle":"2022-04-25T11:30:47.744697Z","shell.execute_reply.started":"2022-04-25T11:27:22.924498Z","shell.execute_reply":"2022-04-25T11:30:47.743506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we've briefly discussed before, the table shown when we fit a model shows us the results after each epoch of training. Remember, an epoch is one complete pass through all of the images in the data. The columns shown are the average loss over the items of the training set, the loss on the validation set, and any metrics that we requested—in this case, the error rate.\n\nRemember that *loss* is whatever function we've decided to use to optimize the parameters of our model. But we haven't actually told fastai what loss function we want to use. So what is it doing? fastai will generally try to select an appropriate loss function based on what kind of data and model you are using. In this case we have image data and a categorical outcome, so fastai will default to using *cross-entropy loss*.","metadata":{"id":"u9uhh-54nqwJ"}},{"cell_type":"markdown","source":"## Training Your Model, and Using It to Clean Your Data","metadata":{"id":"cfEZb-RknkZQ"}},{"cell_type":"markdown","source":"Now let's see whether the mistakes the model is making are mainly difficult cases or are wrong images. To visualize this, we can create a *confusion matrix*:","metadata":{"id":"PPImpXSJnkZR"}},{"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12,12), dpi=60)","metadata":{"id":"PB1HuhutnkZS","outputId":"c5bbcacf-4ab1-4aea-c01d-402282c107cb","execution":{"iopub.status.busy":"2022-04-25T11:30:47.747828Z","iopub.execute_input":"2022-04-25T11:30:47.748186Z","iopub.status.idle":"2022-04-25T11:31:20.135585Z","shell.execute_reply.started":"2022-04-25T11:30:47.748136Z","shell.execute_reply":"2022-04-25T11:31:20.134597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oh dear—in this case, a confusion matrix is very hard to read. We have 37 different breeds of pet, which means we have 37×37 entries in this giant matrix! Instead, we can use the `most_confused` method, which just shows us the cells of the confusion matrix with the most incorrect predictions (here, with at least 5 or more):","metadata":{"id":"SUXy5P-lnqwn","outputId":"85c05401-01aa-4dd3-dadb-4140403f1cc0"}},{"cell_type":"code","source":"interp.most_confused(min_val=5)","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:42:54.792904Z","iopub.execute_input":"2022-04-25T11:42:54.793249Z","iopub.status.idle":"2022-04-25T11:43:07.893163Z","shell.execute_reply.started":"2022-04-25T11:42:54.793218Z","shell.execute_reply":"2022-04-25T11:43:07.891962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since we are not pet breed experts, it is hard for us to know whether these category errors reflect actual difficulties in recognizing breeds. So again, we turn to Google. A little bit of Googling tells us that the most common category errors shown here are actually breed differences that even expert breeders sometimes disagree about. So this gives us some comfort that we are on the right track.","metadata":{}},{"cell_type":"markdown","source":"It's helpful to see where exactly our errors are occurring, to see whether they're due to a dataset problem (e.g., images that aren't pets at all, or are labeled incorrectly, etc.), or a model problem (perhaps it isn't handling images taken with unusual lighting, or from a different angle, etc.). To do this, we can sort our images by their *loss*.\n\nThe loss is a number that is higher if the model is incorrect (especially if it's also confident of its incorrect answer), or if it's correct, but not confident of its correct answer. In a bit we'll learn in depth how loss is calculated and used in the training process. For now, `plot_top_losses` shows us the images with the highest loss in our dataset. As the title of the output says, each image is labeled with four things: prediction, actual (target label), loss, and probability. The *probability* here is the confidence level, from zero to one, that the model has assigned to its prediction:","metadata":{"id":"ssRZ43EcnkZS"}},{"cell_type":"code","source":"interp.plot_top_losses(4, figsize=(10,10))","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:31:32.658243Z","iopub.execute_input":"2022-04-25T11:31:32.658595Z","iopub.status.idle":"2022-04-25T11:31:33.465007Z","shell.execute_reply.started":"2022-04-25T11:31:32.658559Z","shell.execute_reply":"2022-04-25T11:31:33.464131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This output shows the images with the highest loss. You could use this to do some data cleaning as well.\n\nThe intuitive approach to doing data cleaning is to do it *before* you train a model. But as you've seen in this case, a model can actually help you find data issues more quickly and easily. So, we normally prefer to train a quick and simple model first, and then use it to help us with data cleaning.\n","metadata":{"id":"PPF1TGO1nkZT"}},{"cell_type":"code","source":"from fastai.vision.widgets import ImageClassifierCleaner","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:33:38.445064Z","iopub.execute_input":"2022-04-25T11:33:38.445388Z","iopub.status.idle":"2022-04-25T11:33:38.453434Z","shell.execute_reply.started":"2022-04-25T11:33:38.445341Z","shell.execute_reply":"2022-04-25T11:33:38.452281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaner = ImageClassifierCleaner(learn)\ncleaner","metadata":{"execution":{"iopub.status.busy":"2022-04-25T11:33:40.802979Z","iopub.execute_input":"2022-04-25T11:33:40.804037Z","iopub.status.idle":"2022-04-25T11:34:44.378645Z","shell.execute_reply.started":"2022-04-25T11:33:40.803987Z","shell.execute_reply":"2022-04-25T11:34:44.377416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide\n# for idx in cleaner.delete(): cleaner.fns[idx].unlink()\n# for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that amongst our \"saint bernard\" is an image that is a small dog. So, we could choose `<Delete>` in the menu under this image. `ImageClassifierCleaner` doesn't actually do the deleting or changing of labels for you; it just returns the indices of items to change. So, for instance, to delete (`unlink`) all images selected for deletion, we would run:\n\n```python\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\n```\n\nTo move images for which we've selected a different category, we would run:\n\n```python\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n```\n\n> Cleaning the data and getting it ready for your model are two of the biggest challenges for data scientists; they say it takes 90% of their time. The fastai library aims to provide tools that make it as easy as possible.\n\nWe'll be seeing more examples of model-driven data cleaning throughout this book. Once we've cleaned up our data, we can retrain our model. Try it yourself, and see if your accuracy improves!","metadata":{}},{"cell_type":"markdown","source":"## Cross-Entropy Loss","metadata":{"id":"aA0sEcvEnqwJ"}},{"cell_type":"markdown","source":"*Cross-entropy loss* is a loss function that is similar to the one we used in the previous chapter, but (as we'll see) has two benefits:\n\n- It works even when our dependent variable has more than two categories.\n- It results in faster and more reliable training.\n\nIn order to understand how cross-entropy loss works for dependent variables with more than two categories, we first have to understand what the actual data and activations that are seen by the loss function look like.","metadata":{"id":"Zud3wAgonqwK"}},{"cell_type":"markdown","source":"### Viewing Activations and Labels","metadata":{"id":"gjH3Ij_knqwK"}},{"cell_type":"markdown","source":"Let's take a look at the activations of our model. To actually get a batch of real data from our `DataLoaders`, we can use the `one_batch` method:","metadata":{"id":"gpiBTBbpnqwL"}},{"cell_type":"code","source":"x,y = dls.one_batch()","metadata":{"id":"6D2Rr8Z9nqwM","execution":{"iopub.status.busy":"2022-04-25T11:36:18.534374Z","iopub.execute_input":"2022-04-25T11:36:18.53477Z","iopub.status.idle":"2022-04-25T11:36:21.727027Z","shell.execute_reply.started":"2022-04-25T11:36:18.534724Z","shell.execute_reply":"2022-04-25T11:36:21.725743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you see, this returns the dependent and independent variables, as a mini-batch. Let's see what is actually contained in our dependent variable:","metadata":{"id":"fnW6c8ZgnqwM"}},{"cell_type":"code","source":"y","metadata":{"id":"lUm2HE1rnqwM","outputId":"520f42ce-de9e-4c95-e55f-36416faddbb9","execution":{"iopub.status.busy":"2022-04-25T11:36:25.348406Z","iopub.execute_input":"2022-04-25T11:36:25.349535Z","iopub.status.idle":"2022-04-25T11:36:25.36401Z","shell.execute_reply.started":"2022-04-25T11:36:25.349469Z","shell.execute_reply":"2022-04-25T11:36:25.362828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our batch size is 64, so we have 64 rows in this tensor. Each row is a single integer between 0 and 36, representing our 37 possible pet breeds. We can view the predictions (that is, the activations of the final layer of our neural network) using `Learner.get_preds`. This function either takes a dataset index (0 for train and 1 for valid) or an iterator of batches. Thus, we can pass it a simple list with our batch to get our predictions. It returns predictions and targets by default, but since we already have the targets, we can effectively ignore them by assigning to the special variable `_`:","metadata":{"id":"hr8HJBmUnqwN"}},{"cell_type":"code","source":"preds,_ = learn.get_preds(dl=[(x,y)])\npreds[0]","metadata":{"id":"_wViQ5ISnqwN","outputId":"5e16589a-f2ac-4a30-f5b9-7564ae31f29e","execution":{"iopub.status.busy":"2022-04-25T11:36:31.086898Z","iopub.execute_input":"2022-04-25T11:36:31.087203Z","iopub.status.idle":"2022-04-25T11:36:31.212766Z","shell.execute_reply.started":"2022-04-25T11:36:31.087173Z","shell.execute_reply":"2022-04-25T11:36:31.211725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The actual predictions are 37 probabilities between 0 and 1, which add up to 1 in total:","metadata":{"id":"L-kKaUPVnqwN"}},{"cell_type":"code","source":"len(preds[0]),preds[0].sum()","metadata":{"id":"yCMUwHfxnqwN","outputId":"690137fd-52fa-44c9-e04c-6feaf2475dd2","execution":{"iopub.status.busy":"2022-04-25T11:36:36.379787Z","iopub.execute_input":"2022-04-25T11:36:36.383585Z","iopub.status.idle":"2022-04-25T11:36:36.394525Z","shell.execute_reply.started":"2022-04-25T11:36:36.383522Z","shell.execute_reply":"2022-04-25T11:36:36.393225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To transform the activations of our model into predictions like this, we used something called the *softmax* activation function.","metadata":{"id":"j0rj84zvnqwN"}},{"cell_type":"markdown","source":"### Softmax","metadata":{"id":"UyKkjKPznqwO"}},{"cell_type":"markdown","source":"In our classification model, we use the softmax activation function in the final layer to ensure that the activations are all between 0 and 1, and that they sum to 1.\n\nSoftmax is similar to the sigmoid function, which we saw earlier. As a reminder sigmoid looks like this:","metadata":{"id":"1ELBBafJnqwO"}},{"cell_type":"code","source":"plot_function(torch.sigmoid, min=-4,max=4)","metadata":{"id":"UqcKNgO5nqwP","outputId":"e6ad7509-6fa3-418d-ac99-e438f69e7784","execution":{"iopub.status.busy":"2022-04-25T11:36:44.744993Z","iopub.execute_input":"2022-04-25T11:36:44.745311Z","iopub.status.idle":"2022-04-25T11:36:44.993589Z","shell.execute_reply.started":"2022-04-25T11:36:44.745278Z","shell.execute_reply":"2022-04-25T11:36:44.992483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can apply this function to a single column of activations from a neural network, and get back a column of numbers between 0 and 1, so it's a very useful activation function for our final layer.\n\nNow think about what happens if we want to have more categories in our target (such as our 37 pet breeds). That means we'll need more activations than just a single column: we need an activation *per category*. We can create, for instance, a neural net that predicts 3s and 7s that returns two activations, one for each class—this will be a good first step toward creating the more general approach. Let's just use some random numbers with a standard deviation of 2 (so we multiply `randn` by 2) for this example, assuming we have 6 images and 2 possible categories (where the first column represents 3s and the second is 7s):","metadata":{"id":"mtuRCh-nnqwQ"}},{"cell_type":"code","source":"#hide\ntorch.random.manual_seed(42);","metadata":{"id":"YmCm2KTmnqwR","execution":{"iopub.status.busy":"2022-04-25T11:36:48.766026Z","iopub.execute_input":"2022-04-25T11:36:48.766402Z","iopub.status.idle":"2022-04-25T11:36:48.775193Z","shell.execute_reply.started":"2022-04-25T11:36:48.766361Z","shell.execute_reply":"2022-04-25T11:36:48.774285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acts = torch.randn((6,2))*2\nacts","metadata":{"id":"_xrFcMmnnqwR","outputId":"2f2a2f0b-64c8-411f-8416-70a2a23a68f0","execution":{"iopub.status.busy":"2022-04-25T11:36:53.176204Z","iopub.execute_input":"2022-04-25T11:36:53.176574Z","iopub.status.idle":"2022-04-25T11:36:53.18769Z","shell.execute_reply.started":"2022-04-25T11:36:53.176532Z","shell.execute_reply":"2022-04-25T11:36:53.186185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can't just take the sigmoid of this directly, since we don't get rows that add to 1 (i.e., we want the probability of being a 3 plus the probability of being a 7 to add up to 1):","metadata":{"id":"b23MVbcBnqwR"}},{"cell_type":"code","source":"acts.sigmoid()","metadata":{"id":"JD5EkNGrnqwR","outputId":"1a048a2b-9146-485d-e26e-ec885109084a","execution":{"iopub.status.busy":"2022-04-25T11:37:04.051604Z","iopub.execute_input":"2022-04-25T11:37:04.051932Z","iopub.status.idle":"2022-04-25T11:37:04.061747Z","shell.execute_reply.started":"2022-04-25T11:37:04.051899Z","shell.execute_reply":"2022-04-25T11:37:04.060412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In MNIST notebook, our neural net created a single activation per image, which we passed through the `sigmoid` function. That single activation represented the model's confidence that the input was a 3. Binary problems are a special case of classification problems, because the target can be treated as a single boolean value, as we did in `mnist_loss`. But binary problems can also be thought of in the context of the more general group of classifiers with any number of categories: in this case, we happen to have two categories. As we saw in the pet breeds classifier, our neural net will return one activation per category.\n\nSo in the binary case, what do those activations really indicate? A single pair of activations simply indicates the *relative* confidence of the input being a 3 versus being a 7. The overall values, whether they are both high, or both low, don't matter—all that matters is which is higher, and by how much.\n\nWe would expect that since this is just another way of representing the same problem, that we would be able to use `sigmoid` directly on the two-activation version of our neural net. And indeed we can! We can just take the *difference* between the neural net activations, because that reflects how much more sure we are of the input being a 3 than a 7, and then take the sigmoid of that:","metadata":{"id":"3KxXTCBynqwR"}},{"cell_type":"code","source":"(acts[:,0]-acts[:,1]).sigmoid()","metadata":{"id":"Ez9czH8WnqwS","outputId":"2ec1c614-00d1-45ae-f543-689ad552f0b2","execution":{"iopub.status.busy":"2022-04-25T11:38:15.448985Z","iopub.execute_input":"2022-04-25T11:38:15.449288Z","iopub.status.idle":"2022-04-25T11:38:15.459445Z","shell.execute_reply.started":"2022-04-25T11:38:15.449256Z","shell.execute_reply":"2022-04-25T11:38:15.458226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The second column (the probability of it being a 7) will then just be that value subtracted from 1. Now, we need a way to do all this that also works for more than two columns. It turns out that this function, called `softmax`, is exactly that:\n\n``` python\ndef softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True)\n```","metadata":{"id":"KQVTk0GanqwS"}},{"cell_type":"markdown","source":"> jargon: Exponential function (exp): Literally defined as `e**x`, where `e` is a special number approximately equal to 2.718. It is the inverse of the natural logarithm function. Note that `exp` is always positive, and it increases _very_ rapidly!","metadata":{"id":"b2jT6n2vnqwS"}},{"cell_type":"markdown","source":"Let's check that `softmax` returns the same values as `sigmoid` for the first column, and those values subtracted from 1 for the second column:","metadata":{"id":"T8EOO1iXnqwT"}},{"cell_type":"code","source":"sm_acts = torch.softmax(acts, dim=1)\nsm_acts","metadata":{"id":"FPdJypGfnqwT","outputId":"455e5053-1a41-4ad4-83b3-eac2f1f51d95","execution":{"iopub.status.busy":"2022-04-25T11:38:31.798008Z","iopub.execute_input":"2022-04-25T11:38:31.798333Z","iopub.status.idle":"2022-04-25T11:38:31.807845Z","shell.execute_reply.started":"2022-04-25T11:38:31.798295Z","shell.execute_reply":"2022-04-25T11:38:31.806722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`softmax` is the multi-category equivalent of `sigmoid`—we have to use it any time we have more than two categories and the probabilities of the categories must add to 1, and we often use it even when there are just two categories, just to make things a bit more consistent. We could create other functions that have the properties that all activations are between 0 and 1, and sum to 1; however, no other function has the same relationship to the sigmoid function, which we've seen is smooth and symmetric. Also, we'll see shortly that the softmax function works well hand-in-hand with the loss function we will look at in the next section.\n\nIf we have three output activations, such as in our bear classifier, calculating softmax for a single bear image would then look like something like below.","metadata":{"id":"ovQhA9FynqwT"}},{"cell_type":"markdown","source":"<img alt=\"Bear softmax example\" width=\"280\" id=\"bear_softmax\" caption=\"Example of softmax on the bear classifier\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00062.png?raw=1\">","metadata":{"id":"TL0VAwuBnqwU"}},{"cell_type":"markdown","source":"What does this function do in practice? Taking the exponential ensures all our numbers are positive, and then dividing by the sum ensures we are going to have a bunch of numbers that add up to 1. The exponential also has a nice property: if one of the numbers in our activations `x` is slightly bigger than the others, the exponential will amplify this (since it grows, well... exponentially), which means that in the softmax, that number will be closer to 1. \n\nIntuitively, the softmax function *really* wants to pick one class among the others, so it's ideal for training a classifier when we know each picture has a definite label. (Note that it may be less ideal during inference, as you might want your model to sometimes tell you it doesn't recognize any of the classes that it has seen during training, and not pick a class because it has a slightly bigger activation score. In this case, it might be better to train a model using multiple binary output columns, each using a sigmoid activation.)\n\nSoftmax is the first part of the cross-entropy loss—the second part is log likelihood. ","metadata":{"id":"7x2lt13dnqwU"}},{"cell_type":"markdown","source":"### Log Likelihood","metadata":{"id":"FgjEf-ijnqwV"}},{"cell_type":"markdown","source":"When we calculated the loss for our MNIST example in the last chapter we used:\n\n```python\ndef mnist_loss(inputs, targets):\n    inputs = inputs.sigmoid()\n    return torch.where(targets==1, 1-inputs, inputs).mean()\n```\n\nJust as we moved from sigmoid to softmax, we need to extend the loss function to work with more than just binary classification—it needs to be able to classify any number of categories (in this case, we have 37 categories). Our activations, after softmax, are between 0 and 1, and sum to 1 for each row in the batch of predictions. Our targets are integers between 0 and 36.  Furthermore, cross-entropy loss generalizes our binary classification loss and allows for more than one correct label per example (which is called multi-label classificaiton, which we will discuss in Chapter 6).\n\nIn the binary case, we used `torch.where` to select between `inputs` and `1-inputs`. When we treat a binary classification as a general classification problem with two categories, it actually becomes even easier, because (as we saw in the previous section) we now have two columns, containing the equivalent of `inputs` and `1-inputs`. Since there is only one correct label per example, all we need to do is select the appropriate column (as opposed to multiplying multiple probabilities). Let's try to implement this in PyTorch. For our synthetic 3s and 7s example, let's say these are our labels:","metadata":{"id":"bE3S7iSwnqwV"}},{"cell_type":"code","source":"targ = tensor([0,1,0,1,1,0])","metadata":{"id":"vC_GhAJQnqwV","execution":{"iopub.status.busy":"2022-04-25T11:39:02.693359Z","iopub.execute_input":"2022-04-25T11:39:02.694112Z","iopub.status.idle":"2022-04-25T11:39:02.69911Z","shell.execute_reply.started":"2022-04-25T11:39:02.694055Z","shell.execute_reply":"2022-04-25T11:39:02.69807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"and these are the softmax activations:","metadata":{"id":"MI3qbR9EnqwW"}},{"cell_type":"code","source":"sm_acts","metadata":{"id":"ChL_THZgnqwW","outputId":"d839043d-3ba6-4a59-f7b9-1b6d2dd6140a","execution":{"iopub.status.busy":"2022-04-25T11:39:04.491923Z","iopub.execute_input":"2022-04-25T11:39:04.492565Z","iopub.status.idle":"2022-04-25T11:39:04.500582Z","shell.execute_reply.started":"2022-04-25T11:39:04.492528Z","shell.execute_reply":"2022-04-25T11:39:04.499456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then for each item of `targ` we can use that to select the appropriate column of `sm_acts` using tensor indexing, like so:","metadata":{"id":"6AekAvpjnqwW"}},{"cell_type":"code","source":"idx = range(6)\nsm_acts[idx, targ]","metadata":{"id":"NlHHdmM2nqwW","outputId":"f5bc0f85-11e7-43fb-d057-f549c585cf4d","execution":{"iopub.status.busy":"2022-04-25T11:39:12.375893Z","iopub.execute_input":"2022-04-25T11:39:12.376197Z","iopub.status.idle":"2022-04-25T11:39:12.385856Z","shell.execute_reply.started":"2022-04-25T11:39:12.376164Z","shell.execute_reply":"2022-04-25T11:39:12.384669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To see exactly what's happening here, let's put all the columns together in a table. Here, the first two columns are our activations, then we have the targets and the row index.  We explain the last column, `result` below:","metadata":{"id":"3IXz-TJYnqwY"}},{"cell_type":"code","source":"#hide_input\nfrom IPython.display import HTML\ndf = pd.DataFrame(sm_acts, columns=[\"3\",\"7\"])\ndf['targ'] = targ\ndf['idx'] = idx\ndf['result'] = sm_acts[range(6), targ]\nt = df.style.hide_index()\n#To have html code compatible with our script\nhtml = t._repr_html_().split('</style>')[1]\nhtml = re.sub(r'<table id=\"([^\"]+)\"\\s*>', r'<table >', html)\ndisplay(HTML(html))","metadata":{"id":"ANf559s6nqwY","outputId":"925ebc40-6629-4c5e-e8d7-0f3c98deafa2","execution":{"iopub.status.busy":"2022-04-25T11:39:26.141097Z","iopub.execute_input":"2022-04-25T11:39:26.14147Z","iopub.status.idle":"2022-04-25T11:39:26.239095Z","shell.execute_reply.started":"2022-04-25T11:39:26.141436Z","shell.execute_reply":"2022-04-25T11:39:26.237934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at this table, you can see that the `result` column can be calculated by taking the `targ` and `idx` columns as indices into the two-column matrix containing the `3` and `7` columns. That's what `sm_acts[idx, targ]` is actually doing.  The really interesting thing here is that this actually works just as well with more than two columns. To see this, consider what would happen if we added an activation column for every digit (0 through 9), and then `targ` contained a number from 0 to 9.","metadata":{"id":"d5Q8NcHbnqwY"}},{"cell_type":"markdown","source":"PyTorch provides a function that does exactly the same thing as `sm_acts[range(n), targ]` (except it takes the negative, because when applying the log afterward, we will have negative numbers), called `nll_loss` (*NLL* stands for *negative log likelihood*):","metadata":{"id":"blDp3UhsnqwZ"}},{"cell_type":"code","source":"-sm_acts[idx, targ]","metadata":{"id":"VzQ-4vwRnqwZ","outputId":"d392f1ae-1ac1-4280-b6fd-5ee497e23b86","execution":{"iopub.status.busy":"2022-04-25T11:39:40.83876Z","iopub.execute_input":"2022-04-25T11:39:40.839126Z","iopub.status.idle":"2022-04-25T11:39:40.851574Z","shell.execute_reply.started":"2022-04-25T11:39:40.839084Z","shell.execute_reply":"2022-04-25T11:39:40.850641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"F.nll_loss(sm_acts, targ, reduction='none')","metadata":{"id":"9jX527fdnqwZ","outputId":"0e1aa31b-efad-4020-fcd5-e02e0e875e24","execution":{"iopub.status.busy":"2022-04-25T11:39:42.951512Z","iopub.execute_input":"2022-04-25T11:39:42.951886Z","iopub.status.idle":"2022-04-25T11:39:42.961393Z","shell.execute_reply.started":"2022-04-25T11:39:42.951853Z","shell.execute_reply":"2022-04-25T11:39:42.960203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Despite its name, this PyTorch function does not take the log. We'll see why in the next section, but first, let's see why taking the logarithm can be useful.","metadata":{"id":"BEOOPDP9nqwZ"}},{"cell_type":"markdown","source":"> warning: Confusing Name, Beware: The nll in `nll_loss` stands for \"negative log likelihood,\" but it doesn't actually take the log at all! It assumes you have _already_ taken the log. PyTorch has a function called `log_softmax` that combines `log` and `softmax` in a fast and accurate way. `nll_loss` is designed to be used after `log_softmax`.","metadata":{"id":"H81RiOvpnqwa"}},{"cell_type":"markdown","source":"#### Taking the Log\n\nRecall that cross entropy loss may involve the multiplication of many numbers.  Multiplying lots of negative numbers together can cause problems like [numerical underflow](https://en.wikipedia.org/wiki/Arithmetic_underflow) in computers.  Therefore, we want to transform these probabilities to larger values so we can perform mathematical operations on them.  There is a mathematical function that does exactly this: the *logarithm* (available as `torch.log`). It is not defined for numbers less than 0, and looks like this between 0 and 1:","metadata":{"id":"PahOJQ-rnqwa"}},{"cell_type":"code","source":"plot_function(torch.log, min=0,max=1, ty='log(x)', tx='x')","metadata":{"id":"JAbVxHBRnqwb","outputId":"dadbf11c-4ae4-417a-f746-5bbdac6b0601","execution":{"iopub.status.busy":"2022-04-25T11:39:52.025583Z","iopub.execute_input":"2022-04-25T11:39:52.025938Z","iopub.status.idle":"2022-04-25T11:39:52.247102Z","shell.execute_reply.started":"2022-04-25T11:39:52.025906Z","shell.execute_reply":"2022-04-25T11:39:52.246085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Additionally, we want to ensure our model is able to detect differences between small numbers.  For example, consider the probabilities of .01 and .001.  Indeed, those numbers are very close together—but in another sense, 0.01 is 10 times more confident than 0.001.  By taking the log of our probabilities, we prevent these important differences from being ignored.","metadata":{"id":"oEfcmDn7nqwc"}},{"cell_type":"markdown","source":"Does \"logarithm\" ring a bell? The logarithm function has this identity:\n\n```\ny = b**a\na = log(y,b)\n```\n\nIn this case, we're assuming that `log(y,b)` returns *log y base b*. However, PyTorch actually doesn't define `log` this way: `log` in Python uses the special number `e` (2.718...) as the base.\n\nPerhaps a logarithm is something that you have not thought about for the last 20 years or so. But it's a mathematical idea that is going to be really critical for many things in deep learning, so now would be a great time to refresh your memory. The key thing to know about logarithms is this relationship:\n\n    log(a*b) = log(a)+log(b)\n\nWhen we see it in that format, it looks a bit boring; but think about what this really means. It means that logarithms increase linearly when the underlying signal increases exponentially or multiplicatively. This is used, for instance, in the Richter scale of earthquake severity, and the dB scale of noise levels. It's also often used on financial charts, where we want to show compound growth rates more clearly. Computer scientists love using logarithms, because it means that multiplication, which can create really really large and really really small numbers, can be replaced by addition, which is much less likely to result in scales that are difficult for our computers to handle.\n\nObserve that the log of a number approaches negative infinity as the number approaches zero.  In our case, since the result relfects the predicted probability of the correct label, we want our loss function to return a small value when the prediction is \"good\" (closer to 1) and a large value when the prediction is \"bad\" (closer to 0).  We can achieve this by taking the negative of the log:","metadata":{"id":"UhRy2ZEWnqwc"}},{"cell_type":"code","source":"plot_function(lambda x: -1*torch.log(x), min=0,max=1, tx='x', ty='- log(x)', title = 'Log Loss when true label = 1')","metadata":{"id":"c4FM0_bnnqwd","outputId":"10c8aed5-aa5f-410d-e711-28169cbdc3aa","execution":{"iopub.status.busy":"2022-04-25T11:40:04.990311Z","iopub.execute_input":"2022-04-25T11:40:04.991001Z","iopub.status.idle":"2022-04-25T11:40:05.212252Z","shell.execute_reply.started":"2022-04-25T11:40:04.990961Z","shell.execute_reply":"2022-04-25T11:40:05.21136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> It's not just computer scientists that love logs! Until computers came along, engineers and scientists used a special ruler called a \"slide rule\" that did multiplication by adding logarithms. Logarithms are widely used in physics, for multiplying very big or very small numbers, and many other fields.","metadata":{"id":"ljIUS6A2nqwe"}},{"cell_type":"markdown","source":"Let's go ahead and update our previous table with an additional column, `loss` to reflect this loss function:","metadata":{"id":"Hz_WXio_nqwe"}},{"cell_type":"code","source":"#hide_input\nfrom IPython.display import HTML\ndf['loss'] = -torch.log(tensor(df['result']))\nt = df.style.hide_index()\n#To have html code compatible with our script\nhtml = t._repr_html_().split('</style>')[1]\nhtml = re.sub(r'<table id=\"([^\"]+)\"\\s*>', r'<table >', html)\ndisplay(HTML(html))","metadata":{"id":"njY7XM0pnqwe","outputId":"72455d8a-223d-4e0f-dac8-192869330ba5","execution":{"iopub.status.busy":"2022-04-25T11:40:39.592949Z","iopub.execute_input":"2022-04-25T11:40:39.593241Z","iopub.status.idle":"2022-04-25T11:40:39.612154Z","shell.execute_reply.started":"2022-04-25T11:40:39.59321Z","shell.execute_reply":"2022-04-25T11:40:39.609913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice how the loss is very large in the third and fourth rows where the predictions are confident and wrong, or in other words have high probabilities on the wrong class.  One benefit of using the log to calculate the loss is that our loss function penalizes predictions that are both confident and wrong.  This kind of penalty works well in practice to aid in more effective model training.  \n\n> There are other loss functions such as [focal loss](https://arxiv.org/pdf/1708.02002.pdf) that allow you control this penalty with a parameter.  We do not discuss that loss function in this book.","metadata":{"id":"k5ljvLdDnqwf"}},{"cell_type":"markdown","source":"We're calculating the loss from the column containing the correct label. Because there is only one \"right\" answer per example, we don't need to consider the other columns, because by the definition of softmax, they add up to 1 minus the activation corresponding to the correct label. As long as the activation columns sum to 1 (as they will, if we use softmax), then we'll have a loss function that shows how well we're predicting each digit.  Therefore, making the activation for the correct label as high as possible must mean we're also decreasing the activations of the remaining columns.  ","metadata":{"id":"cPuLAApTnqwf"}},{"cell_type":"markdown","source":"### Negative Log Likelihood","metadata":{"id":"qxoLovqRnqwg"}},{"cell_type":"markdown","source":"Taking the mean of the negative log of our probabilities (taking the mean of the `loss` column of our table) gives us the *negative log likelihood* loss, which is another name for cross-entropy loss. Recall that PyTorch's `nll_loss` assumes that you already took the log of the softmax, so it doesn't actually do the logarithm for you.","metadata":{"id":"6ng-KCwNnqwg"}},{"cell_type":"markdown","source":"When we first take the softmax, and then the log likelihood of that, that combination is called *cross-entropy loss*. In PyTorch, this is available as `nn.CrossEntropyLoss` (which, in practice, actually does `log_softmax` and then `nll_loss`):","metadata":{"id":"Ku7LS2Afnqwg"}},{"cell_type":"code","source":"loss_func = nn.CrossEntropyLoss()","metadata":{"id":"3DceO9Z2nqwh","execution":{"iopub.status.busy":"2022-04-25T11:40:58.388389Z","iopub.execute_input":"2022-04-25T11:40:58.388969Z","iopub.status.idle":"2022-04-25T11:40:58.394912Z","shell.execute_reply.started":"2022-04-25T11:40:58.388853Z","shell.execute_reply":"2022-04-25T11:40:58.393533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you see, this is a class. Instantiating it gives you an object which behaves like a function:","metadata":{"id":"_LMsg5xynqwh"}},{"cell_type":"code","source":"loss_func(acts, targ)","metadata":{"id":"-gJtgqVMnqwh","outputId":"2162e395-1933-4a5d-f1c2-8f28595afa5b","execution":{"iopub.status.busy":"2022-04-25T11:40:59.621495Z","iopub.execute_input":"2022-04-25T11:40:59.622099Z","iopub.status.idle":"2022-04-25T11:40:59.630764Z","shell.execute_reply.started":"2022-04-25T11:40:59.622059Z","shell.execute_reply":"2022-04-25T11:40:59.629595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All PyTorch loss functions are provided in two forms, the class just shown above, and also a plain functional form, available in the `F` namespace:","metadata":{"id":"Cul_ZqWinqwi"}},{"cell_type":"code","source":"F.cross_entropy(acts, targ)","metadata":{"id":"r-cqzw77nqwi","outputId":"e9847434-aff5-4bef-84d9-7392722df113","execution":{"iopub.status.busy":"2022-04-25T11:41:05.806275Z","iopub.execute_input":"2022-04-25T11:41:05.806759Z","iopub.status.idle":"2022-04-25T11:41:05.814661Z","shell.execute_reply.started":"2022-04-25T11:41:05.806712Z","shell.execute_reply":"2022-04-25T11:41:05.813645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Either one works fine and can be used in any situation. We've noticed that most people tend to use the class version, and that's more often used in PyTorch's official docs and examples, so we'll tend to use that too.\n\nBy default PyTorch loss functions take the mean of the loss of all items. You can use `reduction='none'` to disable that:","metadata":{"id":"mi4-NjWZnqwk"}},{"cell_type":"code","source":"nn.CrossEntropyLoss(reduction='none')(acts, targ)","metadata":{"id":"JFEIg6gMnqwl","outputId":"00de7aa8-70d4-4dc4-f417-626ebe41d6b5","execution":{"iopub.status.busy":"2022-04-25T11:41:14.347059Z","iopub.execute_input":"2022-04-25T11:41:14.347372Z","iopub.status.idle":"2022-04-25T11:41:14.357387Z","shell.execute_reply.started":"2022-04-25T11:41:14.347323Z","shell.execute_reply":"2022-04-25T11:41:14.356257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You will notice these values match the `loss` column in our table exactly.","metadata":{"id":"lXHipVoDnqwl"}},{"cell_type":"markdown","source":"> An interesting feature about cross-entropy loss appears when we consider its gradient. The gradient of `cross_entropy(a,b)` is just `softmax(a)-b`. Since `softmax(a)` is just the final activation of the model, that means that the gradient is proportional to the difference between the prediction and the target. This is the same as mean squared error in regression (assuming there's no final activation function such as that added by `y_range`), since the gradient of `(a-b)**2` is `2*(a-b)`. Because the gradient is linear, that means we won't see sudden jumps or exponential increases in gradients, which should lead to smoother training of models.","metadata":{"id":"6GXs0vLUnqwl"}},{"cell_type":"markdown","source":"We have now seen all the pieces hidden behind our loss function. But while this puts a number on how well (or badly) our model is doing, it does nothing to help us know if it's actually any good. Let's now see some ways to interpret our model's predictions.","metadata":{"id":"p4BYFW3snqwm"}},{"cell_type":"markdown","source":"## Improving Our Model","metadata":{"id":"_JWQNY_hnqwo"}},{"cell_type":"markdown","source":"We will now look at a range of techniques to improve the training of our model and make it better. While doing so, we will explain a little bit more about transfer learning and how to fine-tune our pretrained model as best as possible, without breaking the pretrained weights.\n\nThe first thing we need to set when training a model is the learning rate. We saw in the previous chapter that it needs to be just right to train as efficiently as possible, so how do we pick a good one? fastai provides a tool for this.","metadata":{"id":"LYiIsWAhnqwo"}},{"cell_type":"markdown","source":"### The Learning Rate Finder","metadata":{"id":"EEktJNVpnqwp"}},{"cell_type":"markdown","source":"One of the most important things we can do when training a model is to make sure that we have the right learning rate. If our learning rate is too low, it can take many, many epochs to train our model. Not only does this waste time, but it also means that we may have problems with overfitting, because every time we do a complete pass through the data, we give our model a chance to memorize it.\n\nSo let's just make our learning rate really high, right? Sure, let's try that and see what happens:","metadata":{"id":"uZt2FI2qnqwq"}},{"cell_type":"code","source":"learn = vision_learner(dls_new, resnet34, metrics=error_rate)\nlearn.fine_tune(1, base_lr=0.1)","metadata":{"id":"fYDzFpftnqwq","outputId":"29f07585-64f1-43a6-d93c-65f6590c6d67","execution":{"iopub.status.busy":"2022-04-25T11:44:01.2389Z","iopub.execute_input":"2022-04-25T11:44:01.239253Z","iopub.status.idle":"2022-04-25T11:46:15.174884Z","shell.execute_reply.started":"2022-04-25T11:44:01.239219Z","shell.execute_reply":"2022-04-25T11:46:15.173543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That doesn't look good. Here's what happened. The optimizer stepped in the correct direction, but it stepped so far that it totally overshot the minimum loss. Repeating that multiple times makes it get further and further away, not closer and closer!\n\nWhat do we do to find the perfect learning rate—not too high, and not too low? In 2015 the researcher Leslie Smith came up with a brilliant idea, called the *learning rate finder*. His idea was to start with a very, very small learning rate, something so small that we would never expect it to be too big to handle. We use that for one mini-batch, find what the losses are afterwards, and then increase the learning rate by some percentage (e.g., doubling it each time). Then we do another mini-batch, track the loss, and double the learning rate again. We keep doing this until the loss gets worse, instead of better. This is the point where we know we have gone too far. We then select a learning rate a bit lower than this point. Our advice is to pick either:\n\n- One order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10)\n- The last point where the loss was clearly decreasing \n\nThe learning rate finder computes those points on the curve to help you. Both these rules usually give around the same value. In the first chapter, we didn't specify a learning rate, using the default value from the fastai library (which is 1e-3):","metadata":{"id":"WKuBP6t7nqwr"}},{"cell_type":"code","source":"learn = vision_learner(dls_new, resnet34, metrics=error_rate)\nlr_min,lr_steep = learn.lr_find(suggest_funcs=(minimum, steep))","metadata":{"id":"_xZ4XYXJnqwr","outputId":"ca67f88f-297f-41d5-fe14-c33ea8c5af7d","execution":{"iopub.status.busy":"2022-04-25T11:46:15.177736Z","iopub.execute_input":"2022-04-25T11:46:15.17808Z","iopub.status.idle":"2022-04-25T11:47:07.321742Z","shell.execute_reply.started":"2022-04-25T11:46:15.178036Z","shell.execute_reply":"2022-04-25T11:47:07.320723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}\")","metadata":{"id":"uHbzDP-mnqws","outputId":"350b4a6a-3edf-4a5e-b180-9d17bb5d5566","execution":{"iopub.status.busy":"2022-04-25T11:47:07.323563Z","iopub.execute_input":"2022-04-25T11:47:07.324457Z","iopub.status.idle":"2022-04-25T11:47:07.331846Z","shell.execute_reply.started":"2022-04-25T11:47:07.324393Z","shell.execute_reply":"2022-04-25T11:47:07.330559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see on this plot that in the range 1e-6 to 1e-3, nothing really happens and the model doesn't train. Then the loss starts to decrease until it reaches a minimum, and then increases again. We don't want a learning rate greater than 1e-1 as it will give a training that diverges like the one before (you can try for yourself), but 1e-1 is already too high: at this stage we've left the period where the loss was decreasing steadily.\n\nIn this learning rate plot it appears that a learning rate around 3e-3 would be appropriate, so let's choose that:","metadata":{"id":"9K2fKRsnnqwt"}},{"cell_type":"code","source":"learn = vision_learner(dls_new, resnet34, metrics=error_rate)\nlearn.fine_tune(2, base_lr=3e-3)","metadata":{"id":"kVJBJ5rJnqwu","outputId":"340a80bb-b750-4e3c-901d-69b72829e3df"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Note: Logarithmic Scale: The learning rate finder plot has a logarithmic scale, which is why the middle point between 1e-3 and 1e-2 is between 3e-3 and 4e-3. This is because we care mostly about the order of magnitude of the learning rate.","metadata":{"id":"9BQ2KWCYnqwu"}},{"cell_type":"markdown","source":"It's interesting that the learning rate finder was only discovered in 2015, while neural networks have been under development since the 1950s. Throughout that time finding a good learning rate has been, perhaps, the most important and challenging issue for practitioners. The solution does not require any advanced maths, giant computing resources, huge datasets, or anything else that would make it inaccessible to any curious researcher. Furthermore, Leslie Smith, was not part of some exclusive Silicon Valley lab, but was working as a naval researcher. All of this is to say: breakthrough work in deep learning absolutely does not require access to vast resources, elite teams, or advanced mathematical ideas. There is lots of work still to be done that requires just a bit of common sense, creativity, and tenacity.","metadata":{"id":"xZyGuJmGnqwv"}},{"cell_type":"markdown","source":"Now that we have a good learning rate to train our model, let's look at how we can fine-tune the weights of a pretrained model.","metadata":{"id":"t6kfr7KOnqwv"}},{"cell_type":"markdown","source":"### Unfreezing and Transfer Learning","metadata":{"id":"OXVZXGHTnqww"}},{"cell_type":"markdown","source":"We discussed briefly in the intro how transfer learning works. We saw that the basic idea is that a pretrained model, trained potentially on millions of data points (such as ImageNet), is fine-tuned for some other task. But what does this really mean?\n\nWe now know that a convolutional neural network consists of many linear layers with a nonlinear activation function between each pair, followed by one or more final linear layers with an activation function such as softmax at the very end. The final linear layer uses a matrix with enough columns such that the output size is the same as the number of classes in our model (assuming that we are doing classification).\n\nThis final linear layer is unlikely to be of any use for us when we are fine-tuning in a transfer learning setting, because it is specifically designed to classify the categories in the original pretraining dataset. So when we do transfer learning we remove it, throw it away, and replace it with a new linear layer with the correct number of outputs for our desired task (in this case, there would be 37 activations).\n\nThis newly added linear layer will have entirely random weights. Therefore, our model prior to fine-tuning has entirely random outputs. But that does not mean that it is an entirely random model! All of the layers prior to the last one have been carefully trained to be good at image classification tasks in general. As we saw in the images from the [Zeiler and Fergus paper](https://arxiv.org/pdf/1311.2901.pdf) in the slides, the first few layers encode very general concepts, such as finding gradients and edges, and later layers encode concepts that are still very useful for us, such as finding eyeballs and fur.\n\nWe want to train a model in such a way that we allow it to remember all of these generally useful ideas from the pretrained model, use them to solve our particular task (classify pet breeds), and only adjust them as required for the specifics of our particular task.\n\nOur challenge when fine-tuning is to replace the random weights in our added linear layers with weights that correctly achieve our desired task (classifying pet breeds) without breaking the carefully pretrained weights and the other layers. There is actually a very simple trick to allow this to happen: tell the optimizer to only update the weights in those randomly added final layers. Don't change the weights in the rest of the neural network at all. This is called *freezing* those pretrained layers.","metadata":{"id":"aie6R2ytnqww"}},{"cell_type":"markdown","source":"When we create a model from a pretrained network fastai automatically freezes all of the pretrained layers for us. When we call the `fine_tune` method fastai does two things:\n\n- Trains the randomly added layers for one epoch, with all other layers frozen\n- Unfreezes all of the layers, and trains them all for the number of epochs requested\n\nAlthough this is a reasonable default approach, it is likely that for your particular dataset you may get better results by doing things slightly differently. The `fine_tune` method has a number of parameters you can use to change its behavior, but it might be easiest for you to just call the underlying methods directly if you want to get some custom behavior. Remember that you can see the source code for the method by using the following syntax:\n\n    learn.fine_tune??\n\nSo let's try doing this manually ourselves. First of all we will train the randomly added layers for three epochs, using `fit_one_cycle`. As mentioned in <<chapter_intro>>, `fit_one_cycle` is the suggested way to train models without using `fine_tune`. We'll see why later in the book; in short, what `fit_one_cycle` does is to start training at a low learning rate, gradually increase it for the first section of training, and then gradually decrease it again for the last section of training.","metadata":{"id":"5rB602KQnqww"}},{"cell_type":"code","source":"learn.fine_tune??","metadata":{"id":"fv13xc2Rnqwx","execution":{"iopub.status.busy":"2022-04-25T11:47:21.789781Z","iopub.execute_input":"2022-04-25T11:47:21.790127Z","iopub.status.idle":"2022-04-25T11:47:21.871537Z","shell.execute_reply.started":"2022-04-25T11:47:21.790077Z","shell.execute_reply":"2022-04-25T11:47:21.870617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn = vision_learner(dls_new, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)","metadata":{"id":"Y5ddEVyJnqwy","outputId":"7e43673c-90f1-4470-d2ce-31f139b28fb3","execution":{"iopub.status.busy":"2022-04-25T11:48:18.852349Z","iopub.execute_input":"2022-04-25T11:48:18.852942Z","iopub.status.idle":"2022-04-25T11:51:29.544635Z","shell.execute_reply.started":"2022-04-25T11:48:18.852907Z","shell.execute_reply":"2022-04-25T11:51:29.543575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we'll unfreeze the model:","metadata":{"id":"mGKuzdmInqwy"}},{"cell_type":"code","source":"learn.unfreeze()","metadata":{"id":"1bg5B2X-nqwz","execution":{"iopub.status.busy":"2022-04-25T11:51:29.546793Z","iopub.execute_input":"2022-04-25T11:51:29.547032Z","iopub.status.idle":"2022-04-25T11:51:29.555825Z","shell.execute_reply.started":"2022-04-25T11:51:29.547001Z","shell.execute_reply":"2022-04-25T11:51:29.553036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"and run `lr_find` again, because having more layers to train, and weights that have already been trained for three epochs, means our previously found learning rate isn't appropriate any more:","metadata":{"id":"YYQC55odnqwz"}},{"cell_type":"code","source":"learn.lr_find()","metadata":{"id":"Ze9mfKohnqwz","outputId":"80ec8981-cea3-4811-d9e6-ba9dd1a79e67","execution":{"iopub.status.busy":"2022-04-25T11:51:29.55766Z","iopub.execute_input":"2022-04-25T11:51:29.558392Z","iopub.status.idle":"2022-04-25T11:52:06.920982Z","shell.execute_reply.started":"2022-04-25T11:51:29.558314Z","shell.execute_reply":"2022-04-25T11:52:06.920045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that the graph is a little different from when we had random weights: we don't have that sharp descent that indicates the model is training. That's because our model has been trained already. Here we have a somewhat flat area before a sharp increase, and we should take a point well before that sharp increase—for instance, 1e-5. The point with the maximum gradient isn't what we look for here and should be ignored.\n\nLet's train at a suitable learning rate:","metadata":{"id":"mQavGfrBnqw0"}},{"cell_type":"code","source":"learn.fit_one_cycle(6, lr_max=1e-5)","metadata":{"id":"36LKyFiinqw0","outputId":"6dfb6302-7b3b-48d4-d1be-ed086bd6db5e","execution":{"iopub.status.busy":"2022-04-25T11:53:36.478548Z","iopub.execute_input":"2022-04-25T11:53:36.478881Z","iopub.status.idle":"2022-04-25T12:00:22.285793Z","shell.execute_reply.started":"2022-04-25T11:53:36.478852Z","shell.execute_reply":"2022-04-25T12:00:22.284465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This has improved our model a bit, but there's more we can do. The deepest layers of our pretrained model might not need as high a learning rate as the last ones, so we should probably use different learning rates for those—this is known as using *discriminative learning rates*.","metadata":{"id":"Rg2u_2lwnqw0"}},{"cell_type":"markdown","source":"### Discriminative Learning Rates","metadata":{"id":"TsZ7srLfnqw1"}},{"cell_type":"markdown","source":"Even after we unfreeze, we still care a lot about the quality of those pretrained weights. We would not expect that the best learning rate for those pretrained parameters would be as high as for the randomly added parameters, even after we have tuned those randomly added parameters for a few epochs. Remember, the pretrained weights have been trained for hundreds of epochs, on millions of images.\n\nIn addition, do you remember the images we saw in the slides, showing what each layer learns? The first layer learns very simple foundations, like edge and gradient detectors; these are likely to be just as useful for nearly any task. The later layers learn much more complex concepts, like \"eye\" and \"sunset,\" which might not be useful in your task at all (maybe you're classifying car models, for instance). So it makes sense to let the later layers fine-tune more quickly than earlier layers.\n\nTherefore, fastai's default approach is to use discriminative learning rates. This was originally developed in the ULMFiT approach to NLP transfer learning that we will introduce in next notebook. Like many good ideas in deep learning, it is extremely simple: use a lower learning rate for the early layers of the neural network, and a higher learning rate for the later layers (and especially the randomly added layers). The idea is based on insights developed by [Jason Yosinski](https://arxiv.org/abs/1411.1792), who showed in 2014 that with transfer learning different layers of a neural network should train at different speeds, as seen in the image below.","metadata":{"id":"t4TyTc_znqw1"}},{"cell_type":"markdown","source":"<img alt=\"Impact of different layers and training methods on transfer learning (Yosinski)\" width=\"680\" caption=\"Impact of different layers and training methods on transfer learning (courtesy of Jason Yosinski et al.)\" id=\"yosinski\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00039.png?raw=1\">","metadata":{"id":"xk5ypyVXnqw1"}},{"cell_type":"markdown","source":"fastai lets you pass a Python `slice` object anywhere that a learning rate is expected. The first value passed will be the learning rate in the earliest layer of the neural network, and the second value will be the learning rate in the final layer. The layers in between will have learning rates that are multiplicatively equidistant throughout that range. Let's use this approach to replicate the previous training, but this time we'll only set the *lowest* layer of our net to a learning rate of 1e-6; the other layers will scale up to 1e-4. Let's train for a while and see what happens:","metadata":{"id":"m4pdqfJjnqw1"}},{"cell_type":"code","source":"learn = vision_learner(dls_new, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)\nlearn.unfreeze()\nlearn.fit_one_cycle(8, lr_max=slice(1e-6,1e-4))","metadata":{"id":"R-W5y2qvnqw2","outputId":"d591d4e4-86b5-44e8-9c47-1353619bd642","execution":{"iopub.status.busy":"2022-04-25T12:37:44.924518Z","iopub.execute_input":"2022-04-25T12:37:44.924838Z","iopub.status.idle":"2022-04-25T12:49:29.456783Z","shell.execute_reply.started":"2022-04-25T12:37:44.924806Z","shell.execute_reply":"2022-04-25T12:49:29.455542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now the fine-tuning is working great!\n\nfastai can show us a graph of the training and validation loss:","metadata":{"id":"mjpZtzLinqw2"}},{"cell_type":"code","source":"learn.recorder.plot_loss()","metadata":{"id":"Nymn8SWUnqw2","outputId":"b8343bb2-116e-4f19-c8a4-31a851ee4c78","execution":{"iopub.status.busy":"2022-04-25T12:49:29.460907Z","iopub.execute_input":"2022-04-25T12:49:29.461243Z","iopub.status.idle":"2022-04-25T12:49:29.786508Z","shell.execute_reply.started":"2022-04-25T12:49:29.461198Z","shell.execute_reply":"2022-04-25T12:49:29.78545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the training loss keeps getting better and better. But notice that eventually the validation loss improvement slows, and sometimes even gets worse! This is the point at which the model is starting to over fit. In particular, the model is becoming overconfident of its predictions. But this does *not* mean that it is getting less accurate, necessarily. Take a look at the table of training results per epoch, and you will often see that the accuracy continues improving, even as the validation loss gets worse. In the end what matters is your accuracy, or more generally your chosen metrics, not the loss. The loss is just the function we've given the computer to help us to optimize.","metadata":{"id":"zd7kR8jknqw2"}},{"cell_type":"markdown","source":"Another decision you have to make when training the model is for how long to train for. We'll consider that next.","metadata":{"id":"MQxJtewxnqw3"}},{"cell_type":"markdown","source":"### Selecting the Number of Epochs","metadata":{"id":"-LqJE6pPnqw3"}},{"cell_type":"markdown","source":"Often you will find that you are limited by time, rather than generalization and accuracy, when choosing how many epochs to train for. So your first approach to training should be to simply pick a number of epochs that will train in the amount of time that you are happy to wait for. Then look at the training and validation loss plots, as shown above, and in particular your metrics, and if you see that they are still getting better even in your final epochs, then you know that you have not trained for too long.\n\nOn the other hand, you may well see that the metrics you have chosen are really getting worse at the end of training. Remember, it's not just that we're looking for the validation loss to get worse, but the actual metrics. Your validation loss will first get worse during training because the model gets overconfident, and only later will get worse because it is incorrectly memorizing the data. We only care in practice about the latter issue. Remember, our loss function is just something that we use to allow our optimizer to have something it can differentiate and optimize; it's not actually the thing we care about in practice.\n\nBefore the days of 1cycle training it was very common to save the model at the end of each epoch, and then select whichever model had the best accuracy out of all of the models saved in each epoch. This is known as *early stopping*. However, this is very unlikely to give you the best answer, because those epochs in the middle occur before the learning rate has had a chance to reach the small values, where it can really find the best result. Therefore, if you find that you have overfit, what you should actually do is retrain your model from scratch, and this time select a total number of epochs based on where your previous best results were found.\n\nIf you have the time to train for more epochs, you may want to instead use that time to train more parameters—that is, use a deeper architecture.","metadata":{"id":"V84BEgC7nqw3"}},{"cell_type":"markdown","source":"### Deeper Architectures","metadata":{"id":"9GgZ3I2Unqw3"}},{"cell_type":"markdown","source":"In general, a model with more parameters can model your data more accurately. (There are lots and lots of caveats to this generalization, and it depends on the specifics of the architectures you are using, but it is a reasonable rule of thumb for now.) For most of the architectures that we will be seeing in this book, you can create larger versions of them by simply adding more layers. However, since we want to use pretrained models, we need to make sure that we choose a number of layers that have already been pretrained for us.\n\nThis is why, in practice, architectures tend to come in a small number of variants. For instance, the ResNet architecture that we are using in this chapter comes in variants with 18, 34, 50, 101, and 152 layer, pretrained on ImageNet. A larger (more layers and parameters; sometimes described as the \"capacity\" of a model) version of a ResNet will always be able to give us a better training loss, but it can suffer more from overfitting, because it has more parameters to overfit with.\n\nIn general, a bigger model has the ability to better capture the real underlying relationships in your data, and also to capture and memorize the specific details of your individual images.\n\nHowever, using a deeper model is going to require more GPU RAM, so you may need to lower the size of your batches to avoid an *out-of-memory error*. This happens when you try to fit too much inside your GPU and looks like:\n\n```\nCuda runtime error: out of memory\n```\n\nYou may have to restart your notebook when this happens. The way to solve it is to use a smaller batch size, which means passing smaller groups of images at any given time through your model. You can pass the batch size you want to the call creating your `DataLoaders` with `bs=`.\n\nThe other downside of deeper architectures is that they take quite a bit longer to train. One technique that can speed things up a lot is *mixed-precision training*. This refers to using less-precise numbers (*half-precision floating point*, also called *fp16*) where possible during training. As we are writing these words in early 2020, nearly all current NVIDIA GPUs support a special feature called *tensor cores* that can dramatically speed up neural network training, by 2-3x. They also require a lot less GPU memory. To enable this feature in fastai, just add `to_fp16()` after your `Learner` creation (you also need to import the module).\n\nYou can't really know ahead of time what the best architecture for your particular problem is—you need to try training some. So let's try a ResNet-50 now with mixed precision:","metadata":{"id":"n0SpL9Bpnqw4"}},{"cell_type":"code","source":"from fastai.callback.fp16 import *\nlearn = vision_learner(dls_new, resnet50, metrics=error_rate).to_fp16()\nlearn.fine_tune(6, freeze_epochs=3)","metadata":{"id":"WVW6azyRnqw4","outputId":"55eeea49-8533-4c88-d604-fa114d466582","execution":{"iopub.status.busy":"2022-04-25T12:26:46.409027Z","iopub.execute_input":"2022-04-25T12:26:46.409335Z","iopub.status.idle":"2022-04-25T12:37:12.572951Z","shell.execute_reply.started":"2022-04-25T12:26:46.409305Z","shell.execute_reply":"2022-04-25T12:37:12.571762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You'll see here we've gone back to using `fine_tune`, since it's so handy! We can pass `freeze_epochs` to tell fastai how many epochs to train for while frozen. It will automatically change learning rates appropriately for most datasets.\n\nIn this case, we're not seeing a clear win from the deeper model. This is useful to remember—bigger models aren't necessarily better models for your particular case! Make sure you try small models before you start scaling up.","metadata":{"id":"ItVbHtW5nqw5"}},{"cell_type":"markdown","source":"## Conclusion","metadata":{"id":"EzBVlUVynqw5"}},{"cell_type":"markdown","source":"In this chapter you learned some important practical tips, both for getting your image data ready for modeling (presizing, data block summary) and for fitting the model (learning rate finder, unfreezing, discriminative learning rates, setting the number of epochs, and using deeper architectures). Using these tools will help you to build more accurate image models, more quickly.\n\nWe also discussed cross-entropy loss. This part of the book is worth spending plenty of time on. You aren't likely to need to actually implement cross-entropy loss from scratch yourself in practice, but it's really important you understand the inputs to and output from that function, because it (or a variant of it, as we'll see in the next chapter) is used in nearly every classification model. So when you want to debug a model, or put a model in production, or improve the accuracy of a model, you're going to need to be able to look at its activations and loss, and understand what's going on, and why. You can't do that properly if you don't understand your loss function.\n\nIf cross-entropy loss hasn't \"clicked\" for you just yet, don't worry—you'll get there! First, go back to the last chapter and make sure you really understand `mnist_loss`. Then work gradually through the cells of the notebook for this chapter, where we step through each piece of cross-entropy loss. Make sure you understand what each calculation is doing, and why. Try creating some small tensors yourself and pass them into the functions, to see what they return.\n\nRemember: the choices made in the implementation of cross-entropy loss are not the only possible choices that could have been made. Just like when we looked at regression we could choose between mean squared error and mean absolute difference (L1). If you have other ideas for possible functions that you think might work, feel free to give them a try in this chapter's notebook! (Fair warning though: you'll probably find that the model will be slower to train, and less accurate. That's because the gradient of cross-entropy loss is proportional to the difference between the activation and the target, so SGD always gets a nicely scaled step for the weights.)","metadata":{"id":"gkDAGMAZnqw5"}}]}